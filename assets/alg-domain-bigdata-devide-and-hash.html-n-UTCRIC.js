const e=JSON.parse('{"key":"v-36343a0f","path":"/develop/algorithms-structures/alg-domain/alg-domain-bigdata-devide-and-hash.html","title":"大数据处理 - 分治/hash/排序","lang":"zh-CN","frontmatter":{"date":"2019-07-27T00:00:00.000Z","category":["算法和数据结构"],"tag":["领域算法"],"description":"大数据处理 - 分治/hash/排序 大数据处理思路: 分而治之/Hash映射 + Hash_map统计 + 堆/快速/归并排序。 思路简介 分而治之/hash映射 + hash统计 + 堆/快速/归并排序，说白了，就是先映射，而后统计，最后排序: 分而治之/hash映射: 针对数据太大，内存受限，只能是: 把大文件化成(取模映射)小文件，即16字方针: 大而化小，各个击破，缩小规模，逐个解决 hash_map统计: 当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。 堆/快速排序: 统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP。","head":[["meta",{"property":"og:url","content":"http://www.gavin-james.com/develop/algorithms-structures/alg-domain/alg-domain-bigdata-devide-and-hash.html"}],["meta",{"property":"og:site_name","content":"Gavin James"}],["meta",{"property":"og:title","content":"大数据处理 - 分治/hash/排序"}],["meta",{"property":"og:description","content":"大数据处理 - 分治/hash/排序 大数据处理思路: 分而治之/Hash映射 + Hash_map统计 + 堆/快速/归并排序。 思路简介 分而治之/hash映射 + hash统计 + 堆/快速/归并排序，说白了，就是先映射，而后统计，最后排序: 分而治之/hash映射: 针对数据太大，内存受限，只能是: 把大文件化成(取模映射)小文件，即16字方针: 大而化小，各个击破，缩小规模，逐个解决 hash_map统计: 当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。 堆/快速排序: 统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-12-27T08:55:11.000Z"}],["meta",{"property":"article:author","content":"gavin-james"}],["meta",{"property":"article:tag","content":"领域算法"}],["meta",{"property":"article:published_time","content":"2019-07-27T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-12-27T08:55:11.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"大数据处理 - 分治/hash/排序\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2019-07-27T00:00:00.000Z\\",\\"dateModified\\":\\"2023-12-27T08:55:11.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"gavin-james\\",\\"url\\":\\"http://www.gavin-james.xyz\\"}]}"]]},"headers":[{"level":2,"title":"思路简介","slug":"思路简介","link":"#思路简介","children":[]},{"level":2,"title":"案例分析","slug":"案例分析","link":"#案例分析","children":[{"level":3,"title":"海量日志数据，提取出某日访问百度次数最多的那个IP","slug":"海量日志数据-提取出某日访问百度次数最多的那个ip","link":"#海量日志数据-提取出某日访问百度次数最多的那个ip","children":[]},{"level":3,"title":"寻找热门查询，300万个查询字符串中统计最热门的10个查询","slug":"寻找热门查询-300万个查询字符串中统计最热门的10个查询","link":"#寻找热门查询-300万个查询字符串中统计最热门的10个查询","children":[]},{"level":3,"title":"有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。","slug":"有一个1g大小的一个文件-里面每一行是一个词-词的大小不超过16字节-内存限制大小是1m。返回频数最高的100个词。","link":"#有一个1g大小的一个文件-里面每一行是一个词-词的大小不超过16字节-内存限制大小是1m。返回频数最高的100个词。","children":[]},{"level":3,"title":"海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。","slug":"海量数据分布在100台电脑中-想个办法高效统计出这批数据的top10。","link":"#海量数据分布在100台电脑中-想个办法高效统计出这批数据的top10。","children":[]},{"level":3,"title":"有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。","slug":"有10个文件-每个文件1g-每个文件的每一行存放的都是用户的query-每个文件的query都可能重复。要求你按照query的频度排序。","link":"#有10个文件-每个文件1g-每个文件的每一行存放的都是用户的query-每个文件的query都可能重复。要求你按照query的频度排序。","children":[]},{"level":3,"title":"给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url?","slug":"给定a、b两个文件-各存放50亿个url-每个url各占64字节-内存限制是4g-让你找出a、b文件共同的url","link":"#给定a、b两个文件-各存放50亿个url-每个url各占64字节-内存限制是4g-让你找出a、b文件共同的url","children":[]},{"level":3,"title":"怎么在海量数据中找出重复次数最多的一个?","slug":"怎么在海量数据中找出重复次数最多的一个","link":"#怎么在海量数据中找出重复次数最多的一个","children":[]},{"level":3,"title":"上千万或上亿数据(有重复)，统计其中出现次数最多的前N个数据。","slug":"上千万或上亿数据-有重复-统计其中出现次数最多的前n个数据。","link":"#上千万或上亿数据-有重复-统计其中出现次数最多的前n个数据。","children":[]},{"level":3,"title":"一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。","slug":"一个文本文件-大约有一万行-每行一个词-要求统计出其中最频繁出现的前10个词-请给出思想-给出时间复杂度分析。","link":"#一个文本文件-大约有一万行-每行一个词-要求统计出其中最频繁出现的前10个词-请给出思想-给出时间复杂度分析。","children":[]},{"level":3,"title":"一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。","slug":"一个文本文件-找出前10个经常出现的词-但这次文件比较长-说是上亿行或十亿行-总之无法一次读入内存-问最优解。","link":"#一个文本文件-找出前10个经常出现的词-但这次文件比较长-说是上亿行或十亿行-总之无法一次读入内存-问最优解。","children":[]},{"level":3,"title":"100w个数中找出最大的100个数。","slug":"_100w个数中找出最大的100个数。","link":"#_100w个数中找出最大的100个数。","children":[]}]}],"git":{"createdTime":1703667311000,"updatedTime":1703667311000,"contributors":[{"name":"gavin-james","email":"meaganlindesy1258@gmail.com","commits":1}]},"readingTime":{"minutes":12.93,"words":3879},"filePathRelative":"develop/algorithms-structures/alg-domain/alg-domain-bigdata-devide-and-hash.md","localizedDate":"2019年7月27日","excerpt":"<h1> 大数据处理 - 分治/hash/排序</h1>\\n<blockquote>\\n<p>大数据处理思路: 分而治之/Hash映射 + Hash_map统计 + 堆/快速/归并排序。</p>\\n</blockquote>\\n<h2> 思路简介</h2>\\n<blockquote>\\n<p>分而治之/hash映射 + hash统计 + 堆/快速/归并排序，说白了，就是先映射，而后统计，最后排序:</p>\\n</blockquote>\\n<ul>\\n<li><code>分而治之/hash映射</code>: 针对数据太大，内存受限，只能是: 把大文件化成(取模映射)小文件，即16字方针: 大而化小，各个击破，缩小规模，逐个解决</li>\\n<li><code>hash_map统计</code>: 当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。</li>\\n<li><code>堆/快速排序</code>: 统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP。</li>\\n</ul>","copyright":{"author":"gavin-james","license":"https://github.com/gavin-james/gavin-james.github.io/LICENSE"},"autoDesc":true}');export{e as data};
